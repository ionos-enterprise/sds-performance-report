{
    "files": [
        {
            "title": "null_bio",
            "file": "null_bio.json",
	        "details": "Linux kernel: v5.12.1<br><br> One or more null_blk devices (queue_mode=0  \"bio\") are exported from server to client.<br><br> NVMEoF are configured with the default parameters.<br><br>In the Multiple Disks setting a single separate fio process is accessing each device, while the number of devices is varied.<br><br>In the Multiple Jobs setting a single device is mapped, while the number of fio jobs accessing that device is variated.<br><br>For Dual Path measurement two different policies have been tested:<br>- RNBD - round robin and min-inflight (choose path with minimum inflights)<br>- NVMEoF - round robin and numa<br><br>Tests have been executed on:<br>- 40 CPUs machines with Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz equipped with Mellanox MT27700 Family ConnectX-4 100Gb/s adaptors<br><br> Module parameter always_invalidate is introduced for the security problem discussed in LPC RDMA MC 2019. When always_invalidate=Y, on the server side we invalidate each rdma buffer before we hand it over to RNBD server and then pass it to the block layer. <br><br>Fio profile:<br>[global]<br>bssplit=512/20:1k/16:2k/9:4k/12:8k/19:16k/10:32k/8:64k/4<br>fadvise_hint=0<br>rw=randrw:2<br>direct=1<br>random_distribution=zipf:1.2<br>time_based=1<br>runtime=30<br>ramp_time=10<br>ioengine=libaio<br>iodepth=129<br>iodepth_batch_submit=128<br>iodepth_batch_complete=128<br>numjobs=1<br>group_reporting<br>cpus_allowed_policy=split<br>numa_mem_policy=local<br>",
            "dictionary": {
                "x4_invalidate_N": "<b>always_invalidate=N</b>",
                "x4_invalidate_Y": "<b>always_invalidate=Y</b>"
            },
            "preSelected": [
                ["x4_invalidate_N", "Dual Path", "Multiple Disks"],
                ["x4_invalidate_Y", "Dual Path", "Multiple Disks"]
            ]
        },
        {
            "title": "null_mq",
            "file": "null_mq.json",
	        "details": "Linux kernel: v5.12.1<br><br> One or more null_blk devices (queue_mode=2 \"mq\") are exported from server to client.<br><br> NVMEoF are configured with the default parameters.<br><br>In the Multiple Disks setting a single separate fio process is accessing each device, while the number of devices is varied.<br><br>In the Multiple Jobs setting a single device is mapped, while the number of fio jobs accessing that device is variated.<br><br>For Dual Path measurement two different policies have been tested:<br>- RNBD - round robin and min-inflight (choose path with minimum inflights)<br>- NVMEoF - round robin and numa<br><br>Tests have been executed on:<br>- 40 CPUs machines with Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz equipped with Mellanox MT27700 Family ConnectX-4 100Gb/s adaptors<br><br> Module parameter always_invalidate is introduced for the security problem discussed in LPC RDMA MC 2019. When always_invalidate=Y, on the server side we invalidate each rdma buffer before we hand it over to RNBD server and then pass it to the block layer. <br><br>Fio profile:<br>[global]<br>bssplit=512/20:1k/16:2k/9:4k/12:8k/19:16k/10:32k/8:64k/4<br>fadvise_hint=0<br>rw=randrw:2<br>direct=1<br>random_distribution=zipf:1.2<br>time_based=1<br>runtime=30<br>ramp_time=10<br>ioengine=libaio<br>iodepth=129<br>iodepth_batch_submit=128<br>iodepth_batch_complete=128<br>numjobs=1<br>group_reporting<br>cpus_allowed_policy=split<br>numa_mem_policy=local<br>",
            "dictionary": {
                "x4_invalidate_N": "<b>always_invalidate=N</b>",
                "x4_invalidate_Y": "<b>always_invalidate=Y</b>"
            },
            "preSelected": [
                ["x4_invalidate_N", "Dual Path", "Multiple Disks"],
                ["x4_invalidate_Y", "Dual Path", "Multiple Disks"]
            ]
        }
    ]

}
